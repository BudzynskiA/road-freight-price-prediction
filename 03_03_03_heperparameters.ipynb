{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9a42a5-cfb1-4982-a4c0-7abdbeafbb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg      \tmin      \n",
      "0  \t20    \t0.0660847\t0.0634888\n",
      "1  \t15    \t0.064959 \t0.0630956\n",
      "2  \t17    \t0.0650351\t0.0626793\n",
      "3  \t16    \t0.0637343\t0.0629955\n",
      "4  \t16    \t0.0632778\t0.0629254\n",
      "5  \t14    \t0.0631113\t0.0629067\n",
      "6  \t14    \t0.0632775\t0.0629067\n",
      "7  \t16    \t0.0630837\t0.0629067\n",
      "8  \t16    \t0.0629637\t0.0629067\n",
      "9  \t12    \t0.0632499\t0.0629067\n",
      "10 \t13    \t0.0630356\t0.0627899\n",
      "Best hyperparameters: {'n_estimators': 581, 'learning_rate': 0.14215770603493744, 'max_depth': 4, 'subsample': 0.9811039617263888, 'min_samples_split': 10}\n",
      "Best MAPE score: 0.06267928521999214\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"final_processed_data.csv\", low_memory=False)\n",
    "\n",
    "# Prepare data (selected important features based on previous experiments)\n",
    "FEATURES = ['TOTAL_KM', 'QTY_LOADS', 'QTY_DELIVERIES',\n",
    "            'COD_DP_MEAN_PRICE_PER_KM', 'COD_LP_MEAN_PRICE_PER_KM',\n",
    "            'START_DELIVERY_TIME_MEAN_PRICE_PER_KM', 'ENTRY_WEEKDAY_MEAN_PRICE_PER_KM',\n",
    "            'HU_KM_PERC', 'TEMP_MIN', 'TEMP_MAX']\n",
    "\n",
    "X = df[FEATURES].values\n",
    "y = df['EUR'].values\n",
    "\n",
    "# Set up DEAP evolutionary algorithm\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # minimize MAPE\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Define hyperparameter search space\n",
    "HYPERPARAMETER_BOUNDS = {\n",
    "    'n_estimators': (50, 1000),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'max_depth': (3, 15),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'min_samples_split': (2, 10),\n",
    "}\n",
    "\n",
    "# Function to generate random individual\n",
    "def generate_individual():\n",
    "    return creator.Individual([\n",
    "        np.random.randint(*HYPERPARAMETER_BOUNDS['n_estimators']),\n",
    "        np.random.uniform(*HYPERPARAMETER_BOUNDS['learning_rate']),\n",
    "        np.random.randint(*HYPERPARAMETER_BOUNDS['max_depth']),\n",
    "        np.random.uniform(*HYPERPARAMETER_BOUNDS['subsample']),\n",
    "        np.random.randint(*HYPERPARAMETER_BOUNDS['min_samples_split']),\n",
    "    ])\n",
    "\n",
    "toolbox.register(\"individual\", generate_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Evaluation function for an individual (MAPE with TimeSeriesSplit validation)\n",
    "def eval_gb(individual):\n",
    "    # Ensure hyperparameters meet valid requirements\n",
    "    params = {\n",
    "        'n_estimators': max(50, int(individual[0])),\n",
    "        'learning_rate': min(max(float(individual[1]), 0.01), 0.3),\n",
    "        'max_depth': max(1, int(round(individual[2]))),\n",
    "        'subsample': min(max(float(individual[3]), 0.5), 1.0),\n",
    "        'min_samples_split': max(2, int(round(individual[4]))),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    mape_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        mape_scores.append(mape)\n",
    "\n",
    "    return (np.mean(mape_scores),)\n",
    "\n",
    "toolbox.register(\"evaluate\", eval_gb)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "\n",
    "# Custom mutation function with constraints\n",
    "def custom_mutation(individual, indpb):\n",
    "    bounds = list(HYPERPARAMETER_BOUNDS.values())\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < indpb:\n",
    "            if isinstance(bounds[i][0], int):\n",
    "                individual[i] = int(np.clip(individual[i] + np.random.randint(-50, 50), bounds[i][0], bounds[i][1]))\n",
    "            else:\n",
    "                individual[i] = float(np.clip(individual[i] + np.random.uniform(-0.05, 0.05), bounds[i][0], bounds[i][1]))\n",
    "    return individual,\n",
    "\n",
    "toolbox.register(\"mutate\", custom_mutation, indpb=0.3)\n",
    "\n",
    "# Evolutionary algorithm parameters\n",
    "population_size = 20\n",
    "num_generations = 10\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "population = toolbox.population(n=population_size)\n",
    "hall_of_fame = tools.HallOfFame(1)\n",
    "\n",
    "# Statistics\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"min\", np.min)\n",
    "\n",
    "population, logbook = algorithms.eaSimple(\n",
    "    population, \n",
    "    toolbox, \n",
    "    cxpb=0.6, \n",
    "    mutpb=0.3, \n",
    "    ngen=num_generations, \n",
    "    stats=stats, \n",
    "    halloffame=hall_of_fame,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Output best hyperparameters\n",
    "best_individual = hall_of_fame[0]\n",
    "best_params = {\n",
    "    'n_estimators': int(best_individual[0]),\n",
    "    'learning_rate': best_individual[1],\n",
    "    'max_depth': int(round(best_individual[2])),\n",
    "    'subsample': best_individual[3],\n",
    "    'min_samples_split': int(round(best_individual[4]))\n",
    "}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best MAPE score:\", best_individual.fitness.values[0])\n",
    "\n",
    "# Save logbook for visualization\n",
    "log_df = pd.DataFrame(logbook)\n",
    "log_df.to_csv('logbook.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf16669-8e47-4401-9d69-bff9e8887633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
