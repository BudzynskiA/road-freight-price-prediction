{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da87676-cfb1-4838-9167-bb1bd41c8634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 890, 'learning_rate': 0.02886496196573106, 'max_depth': 6, 'subsample': 0.9711008778424264, 'min_samples_split': 7}\n",
      "Best MAPE: 0.06245690450057456\n",
      "CSV saved to: logbook_smape.csv\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from deap import base, creator, tools\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"final_processed_data.csv\", low_memory=False)\n",
    "\n",
    "# Select features\n",
    "FEATURES = [\n",
    "    \"TOTAL_KM\", \"QTY_LOADS\", \"QTY_DELIVERIES\",\n",
    "    \"COD_DP_MEAN_PRICE_PER_KM\", \"COD_LP_MEAN_PRICE_PER_KM\",\n",
    "    \"START_DELIVERY_TIME_MEAN_PRICE_PER_KM\", \"ENTRY_WEEKDAY_MEAN_PRICE_PER_KM\",\n",
    "    \"HU_KM_PERC\", \"TEMP_MIN\", \"TEMP_MAX\"\n",
    "]\n",
    "\n",
    "# Basic check to catch typos early\n",
    "missing = [c for c in FEATURES if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in final_processed_data.csv: {missing}\")\n",
    "\n",
    "X = df[FEATURES].values\n",
    "y = df[\"EUR\"].values\n",
    "\n",
    "# Define sMAPE (percent)\n",
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    denom = np.where(denom == 0, 1e-12, denom)\n",
    "    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n",
    "\n",
    "# DEAP setup (minimize fitness)\n",
    "if \"FitnessMin\" not in creator.__dict__:\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "if \"Individual\" not in creator.__dict__:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Hyperparameter bounds\n",
    "HYPERPARAMETER_BOUNDS = {\n",
    "    \"n_estimators\": (50, 1000),\n",
    "    \"learning_rate\": (0.01, 0.3),\n",
    "    \"max_depth\": (3, 15),\n",
    "    \"subsample\": (0.5, 1.0),\n",
    "    \"min_samples_split\": (2, 10),\n",
    "}\n",
    "\n",
    "# Random individual\n",
    "def generate_individual():\n",
    "    return creator.Individual([\n",
    "        np.random.randint(*HYPERPARAMETER_BOUNDS[\"n_estimators\"]),\n",
    "        np.random.uniform(*HYPERPARAMETER_BOUNDS[\"learning_rate\"]),\n",
    "        np.random.randint(*HYPERPARAMETER_BOUNDS[\"max_depth\"]),\n",
    "        np.random.uniform(*HYPERPARAMETER_BOUNDS[\"subsample\"]),\n",
    "        np.random.randint(*HYPERPARAMETER_BOUNDS[\"min_samples_split\"]),\n",
    "    ])\n",
    "\n",
    "toolbox.register(\"individual\", generate_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Evaluation: returns fitness (MAPE); stores sMAPE for logging\n",
    "def eval_gb(individual, n_splits=5):\n",
    "    try:\n",
    "        params = {\n",
    "            \"n_estimators\": max(50, int(individual[0])),\n",
    "            \"learning_rate\": float(np.clip(individual[1], 0.01, 0.3)),\n",
    "            \"max_depth\": max(1, int(round(individual[2]))),\n",
    "            \"subsample\": float(np.clip(individual[3], 0.5, 1.0)),\n",
    "            \"min_samples_split\": max(2, int(round(individual[4]))),\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        mape_scores = []\n",
    "        smape_scores = []\n",
    "\n",
    "        for tr_idx, te_idx in tscv.split(X):\n",
    "            X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pr = model.predict(X_te)\n",
    "            mape_scores.append(mean_absolute_percentage_error(y_te, y_pr))\n",
    "            smape_scores.append(smape(y_te, y_pr))\n",
    "\n",
    "        individual.smape = float(np.mean(smape_scores))\n",
    "        return (float(np.mean(mape_scores)),)\n",
    "    except Exception as e:\n",
    "        individual.smape = np.inf\n",
    "        print(f\"[warn] eval_gb failed for {list(individual)}: {repr(e)}\", file=sys.stderr)\n",
    "        return (1e9,)\n",
    "\n",
    "# Register EA operators\n",
    "toolbox.register(\"evaluate\", lambda ind: eval_gb(ind, n_splits=5))\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "\n",
    "def custom_mutation(individual, indpb):\n",
    "    bounds = list(HYPERPARAMETER_BOUNDS.values())\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < indpb:\n",
    "            if isinstance(bounds[i][0], int):\n",
    "                individual[i] = int(np.clip(individual[i] + np.random.randint(-50, 50),\n",
    "                                            bounds[i][0], bounds[i][1]))\n",
    "            else:\n",
    "                individual[i] = float(np.clip(individual[i] + np.random.uniform(-0.05, 0.05),\n",
    "                                              bounds[i][0], bounds[i][1]))\n",
    "    return individual,\n",
    "\n",
    "toolbox.register(\"mutate\", custom_mutation, indpb=0.3)\n",
    "\n",
    "# EA parameters (full run)\n",
    "population_size = 20\n",
    "num_generations = 10\n",
    "cxpb, mutpb = 0.6, 0.3\n",
    "\n",
    "# Initialize population and HOF\n",
    "population = toolbox.population(n=population_size)\n",
    "hof = tools.HallOfFame(1)\n",
    "\n",
    "# Prepare flat CSV (append mode per generation)\n",
    "LOG_PATH = \"logbook_smape.csv\"\n",
    "if os.path.exists(LOG_PATH):\n",
    "    os.remove(LOG_PATH)\n",
    "pd.DataFrame(columns=[\"gen\", \"avg\", \"min\", \"avg_smape\", \"min_smape\", \"nevals\"]).to_csv(LOG_PATH, index=False)\n",
    "\n",
    "# Evolutionary loop with per-generation checkpointing\n",
    "for gen in range(num_generations + 1):\n",
    "    invalid = [ind for ind in population if not ind.fitness.valid]\n",
    "    fits = list(map(toolbox.evaluate, invalid))\n",
    "    for ind, fit in zip(invalid, fits):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    hof.update(population)\n",
    "\n",
    "    mape_vals = np.array([float(ind.fitness.values[0]) for ind in population], dtype=float)\n",
    "    smape_vals = np.array([getattr(ind, \"smape\", np.nan) for ind in population], dtype=float)\n",
    "\n",
    "    row = pd.DataFrame([{\n",
    "        \"gen\": gen,\n",
    "        \"avg\": float(np.nanmean(mape_vals)),\n",
    "        \"min\": float(np.nanmin(mape_vals)),\n",
    "        \"avg_smape\": float(np.nanmean(smape_vals)),\n",
    "        \"min_smape\": float(np.nanmin(smape_vals)),\n",
    "        \"nevals\": int(len(invalid))\n",
    "    }])\n",
    "    row.to_csv(LOG_PATH, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    if gen == num_generations:\n",
    "        break\n",
    "\n",
    "    # Variation\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    for c1, c2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if np.random.rand() < cxpb:\n",
    "            toolbox.mate(c1, c2)\n",
    "            if hasattr(c1, \"smape\"): delattr(c1, \"smape\")\n",
    "            if hasattr(c2, \"smape\"): delattr(c2, \"smape\")\n",
    "            if hasattr(c1.fitness, \"values\"): del c1.fitness.values\n",
    "            if hasattr(c2.fitness, \"values\"): del c2.fitness.values\n",
    "\n",
    "    for m in offspring:\n",
    "        if np.random.rand() < mutpb:\n",
    "            toolbox.mutate(m)\n",
    "            if hasattr(m, \"smape\"): delattr(m, \"smape\")\n",
    "            if hasattr(m.fitness, \"values\"): del m.fitness.values\n",
    "\n",
    "    population = offspring\n",
    "\n",
    "# Report best\n",
    "best = hof[0]\n",
    "best_params = {\n",
    "    \"n_estimators\": int(best[0]),\n",
    "    \"learning_rate\": float(best[1]),\n",
    "    \"max_depth\": int(round(best[2])),\n",
    "    \"subsample\": float(best[3]),\n",
    "    \"min_samples_split\": int(round(best[4])),\n",
    "}\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best MAPE:\", float(best.fitness.values[0]))\n",
    "print(\"CSV saved to:\", LOG_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
